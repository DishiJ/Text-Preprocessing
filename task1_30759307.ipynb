{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: DISHI JAIN\n",
    "#### Student ID: 30759307\n",
    "\n",
    "\n",
    "Libraries used:\n",
    "* langid (for verifying the language of a string)\n",
    "* os (for accessing the files in the operating system) \n",
    "* re (for regular expression, included in Anaconda Python 3.6) \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment focuses on the extracting data from semi structured text files. From the given dataset containing multiple text files, we have to extract \"id\" , \"date\" and \"text\" of a tweet. This can be done using Regular Expression. After extraction of this information, we need to transform this data to an xml format. \n",
    "\n",
    "More details of this task will be seen in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Libraries importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import langid\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the entire data into a string called data\n",
    "data = \"\"\n",
    "\n",
    "#path where all files are present\n",
    "Path = \"FIT5196_A1/\"\n",
    "filelist = os.listdir(Path)\n",
    "for i in filelist:\n",
    "    #to get all files ending with .txt\n",
    "    if i.endswith(\".txt\"):  \n",
    "        with open(Path + i, 'r', encoding=\"utf8\") as start_object:\n",
    "            data = data + start_object.read() \n",
    "\n",
    "start_values = data.split(\"\\\"data\\\":\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output of the first step, we will get an object which contains the different files loaded. These individual files can be accessed by indexing this object. Hence every operation can now be performed by indexing this object.\n",
    "\n",
    "Encoding of utf-8 is used for this task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a final dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will first iterate through each file using start_values. Inside this we will use the Regular Expression to extract only the ids, dates and text using thee different regular expressions. \n",
    "\n",
    "Implementation works as follows - \n",
    "\n",
    "* Iterating through start_values to get one string for one file containing all the data for that file\n",
    "* Using the regular expression \\\"id\\\":\\\"\\d{19}\\b to extract the id from the entire string. As we know that an id is a term of 19 digits hence I have used the regular expression in a way that will extract the 19 digits after it sees a term like \"id\":   \n",
    "Hence this will help to extract all the ids\n",
    "* Next the regular expression used to extract the text or the tweet is  - text\":\"[^\"]*    This regular expression will help us to extract the text part of every tweet. It will work on the string and look for the term \"text\" . After this it will extract all the characters hence the * in the RE until a \" is seen hence the [^\"] prt in the RE. This [^\"] tells the RE to see a character that is anything but \"    By ^ we mean a negation. Hence after this all the text part is extracted. Now another list is created which contains the text as is it hence removing the \"text\" part. This is done using slicing concept. This is done on the step - fd.append(k[7:]) in the below code. \n",
    "* Next we create another regular expression which will extract the dates of the tweets. The RE used is - created_at\":\"[^T]*\n",
    "Similar concept like RE for text is used here. This RE will extract all the created date part from the string which starts with the term created_at and ends when the character T is seen. Hence it will take all the character after created_at until the first occurence of T. This is used to extract the dates only from the string. Also each string contains date multiple times. Hence the dates are extracted multiple times by this RE. This is handled using slicing again. First I check if the extracted part's length is greater that 0 or not. Then using slicing date = e[0][13:] I have created a date variable which will contain the date as a string just once. \n",
    "* Next the zip function is used between the list of ids and the list of texts. Then the text is checked for english using the library langid. If the text is english then only a new dictionary is created that takes the id as its key and the text as its value. Now this dictionary is appended to a list called new_list. Hence we create a list of dictionaries in this step. \n",
    "* Next we create another dictionary called fcd which contains the date as its key and the list of dictionaries(created above) as its value. If the date or the key is not in the dictionary fcd then the list becomes the value else if the date or key is already in the dictionary then the list is appended to the value of the date or key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = []\n",
    "fd = []\n",
    "fcd = {}\n",
    "date = ''\n",
    "ft = []\n",
    "new_list = []\n",
    "\n",
    "#iterating for each text file\n",
    "for i in start_values: \n",
    "    fc = []\n",
    "    fd = []\n",
    "    new_list = []\n",
    "    \n",
    "    #regular expression to extract the ids including the term \"id\"\n",
    "    c = re.findall(r'\\\"id\\\":\\\"\\d{19}\\b',i)\n",
    "    for j in c:\n",
    "        #creating a list fc containing only the ids \n",
    "        fc = fc + re.findall(r'\\d{19}\\b',j)\n",
    "        \n",
    "    #regular expression to extract the text part of the string \n",
    "    d = re.findall(r'text\":\"[^\"]*',i)\n",
    "    for k in d:\n",
    "        \n",
    "        #using slicing to remove \"text:\" terms from the extrcated tweets hence keeping just the tweet \n",
    "        fd.append(k[7:])\n",
    "        \n",
    "    #regular expression to extract the created date including the \"created_at\" term\n",
    "    e = re.findall(r'created_at\":\"[^T]*',i)\n",
    "    if len(e) > 0:\n",
    "        #date variable to store the date as a string just once using indexing and slicing \n",
    "        date = e[0][13:]\n",
    "        \n",
    "    #zip of the lists fc and fd containing ids and texts only\n",
    "    for i,j in zip(fc,fd):\n",
    "        new_dicti = {}\n",
    "        \n",
    "        #check for tweet laguage. If it is english then only it is used\n",
    "        if langid.classify(j)[0] == 'en':\n",
    "            \n",
    "            j = j.replace('&' , \"&amp;\")\n",
    "            j = j.replace(\"<\" , \"&lt;\")\n",
    "            j = j.replace(\">\" , \"&gt;\")\n",
    "            j = j.replace(\"'\" , \"&apos;\")\n",
    "            j = j.replace('\"' , \"&quot;\")\n",
    "            \n",
    "            #creating dictionary that has id as key and text as value\n",
    "            new_dicti[i] = j\n",
    "            #appending the dictionary to a new list\n",
    "            new_list.append(new_dicti)\n",
    "                \n",
    "    if len(e) > 0: \n",
    "        \n",
    "        #creating a final dictionary that has date as id and a list of dictionaries created above as value\n",
    "        if date in fcd.keys():\n",
    "            for k in new_list:\n",
    "                fcd[date].append(k)\n",
    "        else:\n",
    "            fcd[date] = new_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence after this step we get a dictionary whose key is a date and value is a list of dictionaries(Where the dictionary has key as id and value as text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a string in the desired output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final step, a string is created that will be in the required format as the sample.xml file. This string is called the final_line which will be used to write into the 30759307.xml file. To get this final string we iterate into the final dictionary created above and use its key and values part to create the string. Tags like <data> , <tweet>, <tweets> are also used and written into this string along with their closing tags. \n",
    "    \n",
    "The final string is then written onto the 30759307.xml file using open() and write(). \n",
    "The write() takes up only a string value which will be written onto the file. Encoding of utf-8 is used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty string created\n",
    "final_line = ''\n",
    "final_line = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' + '\\n<data>' \n",
    "\n",
    "#iterating inside the final dictionary to get the keys and values\n",
    "for i in fcd.keys():\n",
    "    \n",
    "    #adding the date into the final_line\n",
    "    final_line = final_line + '\\n<tweets date=\"' + i + '\">'\n",
    "    \n",
    "    #looping in the values part of the final dictionary which is a list\n",
    "    for j in fcd[i]:\n",
    "        \n",
    "        #getting the id(key) and text(value) of each dictionary inside the list to add to the string\n",
    "        for k,m in j.items():\n",
    "            final_line = final_line + '\\n<tweet id=\"' + k + '\">' + m + '</tweet>'\n",
    "            \n",
    "    #closing the tweets tag to mark end of tweets of a day\n",
    "    final_line = final_line + '\\n</tweets>'\n",
    "\n",
    "#closing the data tag to mark the end of the file\n",
    "final_line = final_line + '\\n</data>'\n",
    "\n",
    "#replacing the \\\\n with \\n\n",
    "final_line = final_line.replace('\\\\n','\\n')\n",
    "\n",
    "\n",
    "\n",
    "#opening a file 30759307.xml to write in utf8 encoding\n",
    "f = open(\"30759307.xml\", \"w\", encoding=\"utf8\")\n",
    "\n",
    "#writing the final_line to this xml file\n",
    "f.write(final_line)\n",
    "\n",
    "#closing the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assessment measured the understanding of basic text file processing techniques in the Python programming language. The main outcomes achieved while applying these techniques were:\n",
    "\n",
    "* In the first part of reading files all the text files were read that were present in the current directory using os library.\n",
    "* In the next part of this task regular expression has been used to extract the useful information hence only the date, id, text is extracted for the tweet. In this step the main task was to create a dictionary containing keys as dates and values as a list of dictionaries where each dictionary has key as id and value as text. \n",
    "* Finally the dictionary was iterated and used to create a final string that contains the tags and desired format as in the output file sample.xml. After this the string is written onto a file called 30759307.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
