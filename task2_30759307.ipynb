{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: DISHI JAIN\n",
    "#### Student ID: 30759307\n",
    "\n",
    "\n",
    "Libraries used:\n",
    "* langid (for verifying the language of a string)\n",
    "* pandas (for creating dataframe and using pandas features on dataframe) \n",
    "* nltk (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "* nltk.probability (for using FreqDist functionality)\n",
    "* nltk.stem (for stemming words using PorterStemmer)\n",
    "* nltk.util (for using ngrams)\n",
    "* nltk.metrics (for using BigramAssocMeasures in collocations)\n",
    "* itertools (for using chain functionality)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment focuses on writing a Python code to preprocess a set of tweets and convert them into numerical representations (which are suitable for input into recommender-systems/ information-retrieval algorithms). This task involves extracting the required data i.e. the tweets daywise where each day is represented by the sheets in the excel file. For each day after extracting the english tweet text certain steps involving tokenization, stopwords removal, removal of tokens with length less than 3, removal of context dependent tokens and rare tokens are involved. Then unigrams, bigrams, vocab and vector files need to be generated. \n",
    "\n",
    "More details of this task will be seen in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Libraries importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporing the required libraries\n",
    "import langid\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from itertools import chain\n",
    "\n",
    "stem = PorterStemmer()\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is as follows - \n",
    "\n",
    "* In this task the xml file 30759307 is loaded into an excel object called excel using pandas and ExcelFile function. This excel object then iterates through the sheets of the excel file using sheet_names functionality. \n",
    "* For each sheet now we perform ceratin operations for cleaning of the file. First the columns and rows with NA as their values in each cell i.e. in all the cells are removed as they are of no use to us. This is done by using dropna and arguments as axis and how. \n",
    "* Next I have resetted the index of the dataframe using reset_index functionality. After doing this the original indexes are obtained as a separate column. Hence to remove them I have used drop function and given the column name 'index' to it along with the axis. Inplace = True is used to make the changes permanent to the dataframe. \n",
    "* Next I check if the term 'text' is present in the columns of the dataframe or not. If it is not present in the columns then I create a header variable corresponding to the first row of the dataframe. This will contain the values text,id and date. Then I create a new dataframe with this header as the column names and the rows as the previous dataframe from 2nd row. If however the term 'text' is present already in the column, then I don't create any new dataframe. \n",
    "* Next I iterate through the column text and check if the values are of string type. If not a string I have removed the row from the dataframe. \n",
    "* Next I iterate through the text column of the dataframe to check if the text is in english or not. If the text is in english then I use a dictionary called d and give the date as its key and the remaining text as its value. The text part will get appended. Hence a final dictionary d is created with date as its key and tweet's text as it's values where each text is separated by a newline character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ''\n",
    "excel = pd.ExcelFile('30759307.xlsx')\n",
    "\n",
    "d = {}\n",
    "\n",
    "#transversing through sheets in the excel object\n",
    "for x in excel.sheet_names:\n",
    "    df = excel.parse(x)\n",
    "    \n",
    "    #dropping the rows with NA as all its vales\n",
    "    df = df.dropna(0, how = 'all')\n",
    "    #dropping the columns with NA as all its vales\n",
    "    df = df.dropna(1, how = 'all')\n",
    "    \n",
    "    #resetting the index\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    #removing the generated column 'index'\n",
    "    df.drop('index',axis = 1,inplace=True)   \n",
    "    \n",
    "    #checking for term 'text' in columns\n",
    "    if 'text' not in df.columns:\n",
    "        \n",
    "        #taking the first row as headers\n",
    "        headers = df.iloc[0]\n",
    "        #creating a new dataframe\n",
    "        new_df  = pd.DataFrame(df.values[1:], columns=headers)\n",
    "        \n",
    "        #iterating through the text column\n",
    "        for i in new_df.text.values:\n",
    "            #if type of text is not string then removing it\n",
    "            if type(i) != str:\n",
    "                index_names = new_df[ new_df['text'] == i ].index \n",
    "                new_df.drop(index_names, inplace = True) \n",
    "        \n",
    "        #iterating through the text column        \n",
    "        for i in new_df.text.values:\n",
    "            #check for language of text string\n",
    "            if langid.classify(i)[0] == 'en':                \n",
    "                if type(new_df.text.values) != str:\n",
    "                    if x in d:\n",
    "                        \n",
    "                        #appending the string to dictionary value\n",
    "                        d[x] = d[x] + '\\n' + str(i)\n",
    "                    else:\n",
    "                        d[x] = str(i)\n",
    "                else:\n",
    "                    if x in d:\n",
    "                        d[x] = d[x] + '\\n' + i\n",
    "                    else:\n",
    "                        d[x] = i\n",
    "                    \n",
    "    else:\n",
    "        #iterating through the text column\n",
    "        for i in df.text.values:\n",
    "            #if type of text is not string then removing it\n",
    "            if type(i) != str:\n",
    "                print(i)\n",
    "                index_names = df[ df['text'] == i ].index \n",
    "                print(index_names)\n",
    "                df.drop(index_names, inplace = True) \n",
    "                \n",
    "        #iterating through the text column       \n",
    "        for i in df.text.values:\n",
    "            \n",
    "            #check for language of text string\n",
    "            if langid.classify(i)[0] == 'en':\n",
    "                if type(new_df.text.values) != str:\n",
    "                    if x in d:\n",
    "                        \n",
    "                        #appending the string to dictionary value\n",
    "                        d[x] = d[x] + '\\n' + str(i)\n",
    "                    else:\n",
    "                        d[x] = str(i)\n",
    "                else:\n",
    "                    if x in d:\n",
    "                        d[x] = d[x] + '\\n' + i\n",
    "                    else:\n",
    "                        d[x] = i\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we get a dictionary that has key as date and value as a string containing all the tweets for that date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we create tokens using the RegularExpression given to us. We iterate it through each value in the above created dictionary and create tokens of it after converting it into lowercase. This data is then stored in another dictionary called tokens_dicti which has date as key and list of tokens as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary for storing tokens date wise into a list\n",
    "tokens_dicti = {}\n",
    "\n",
    "#generating a RegexpTokenizer object with the RE as the one given to us\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "#iterating inside the dictionary created before to create tokens of each value after conversion to lowercase\n",
    "for key,val in d.items():\n",
    "    tokens_dicti[key] = tokenizer.tokenize(d[key].lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process bigrams are generated from the tokens given to us date wise. We get the first 200 bigrams using the nbest feature. Also we use the pmi measure collocations while creating the bigrams to get the most meaningful bigrams. Then I have created a dictionary which will contain the dates as keys and bigrams generated into a list. \n",
    "\n",
    "Making use of this final dictionary then the file sample_100bi.txt is created. A string is then generated that will be in the same format as expected, where each row represents a document and then each document has a list of bigrams along with its frequency/count in the document. Only the first 100 bigrams are taken for this task. Finally the string is written to the output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100bi.txt\n",
    "\n",
    "bi_final = {}\n",
    "\n",
    "#iterating in the dictionary created above\n",
    "for i in tokens_dicti:\n",
    "    \n",
    "    #Creating bigrams from values of tokens_dicti\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(tokens_dicti[i])\n",
    "    \n",
    "    #getting top 200 bigrams\n",
    "    bigram_list = finder.nbest(bigram_measures.pmi,200)\n",
    "    #selecting only the top 100 bigrams and inserting into value of bi_final\n",
    "    bi_final[i] = (sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:100])\n",
    "\n",
    "    \n",
    "final_string_bi = ''\n",
    "\n",
    "#iterating in bi_final dictionary to create the string in the required format\n",
    "for i in bi_final:\n",
    "    final_string_bi = final_string_bi + str(i) + ':' +  str(bi_final[i]) + '\\n'\n",
    "\n",
    "#writing the string into the file sample_100bi.txt\n",
    "f2 = open(\"30759307_100bi.txt\" , 'w', encoding = 'utf8')\n",
    "f2.write(final_string_bi)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we have successfully generated a bigram text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation is as follows - \n",
    "\n",
    "* In this below step I have first read the stopword file given to us and generated a list stopwords_list containing all the stopwords. I have then created a set using this stopwords_list to get unique stopwords.\n",
    "* Then I have removed all the stopwords from the tokens_dicti dictionary using list comprehension and stored the final terms in stopwords_removal_final where key is date and value is a list of tokens after removal of stopwords\n",
    "* Then I have removed the tokens whose lenth is less than 3. For this I have created another dictionary called greater_than_3 which contains key as date and values are appended from previous step and after removing tokens with lenth less than 3.\n",
    "* Then I have handled rare tokens removal and context dependent tokens removal. For this I have used the dictionary created in above step and have given it as an argument to chain.from_iterable function. I have also used list comprehension inside it and FreqDist which calculates the document frequency of the tokens. This is stored in document_freq. Next I have iterated inside this to get keys(tokens) and values(document frequency). If the value is greater than 60 or less than 5 then a final_document_freq dictionary is created which contains the valid tokens only. \n",
    "* Then I have created another dictionary called rare_removal which will contain the dates as keys and tokens as values. These tokens are the ones which have undergone - stopwords removal, length less than 3 removal, rare tokens removal and context dependent removal. \n",
    "* After this I have stemmed the words using PortStemmer() stem. Finally the dictionary stemmed_words is created containing dates as keys and stemmed versions of valid tokens. \n",
    "* Now to get the frequency of each token date wise I have used FreqDist in stemmed_words and used most_common functionality to get top 100 stemmed tokens date wise. This is finally stored in uni_final\n",
    "* Now using the dictionary uni_final I have written the data as the required format into the string final_string_uni. This string is then written to the file sample_100uni.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 uni.txt\n",
    "\n",
    "#FINAL DISHI JAIN\n",
    "\n",
    "\n",
    "#stopwards removal\n",
    "with open('stopwords_en.txt' , 'r') as f:\n",
    "    stopwords_list = f.read().splitlines() \n",
    "    \n",
    "    \n",
    "stopwords_removal_final = {}\n",
    "#creating set of stopwords\n",
    "stopwords_set = set(stopwords_list)\n",
    "#iterating inside tokens_dicti to remove stopwords\n",
    "for i in tokens_dicti:\n",
    "    stopwords_removal_final[i] = [w for w in tokens_dicti[i] if w not in stopwords_set]\n",
    "    \n",
    "\n",
    "#length < 3 removal\n",
    "greater_than_3 = {}\n",
    "for i in stopwords_removal_final:\n",
    "    for j in stopwords_removal_final[i]:\n",
    "        #check for length of the tokens\n",
    "        if len(j) >= 3:\n",
    "            if i in greater_than_3:\n",
    "                greater_than_3[i].append(j)\n",
    "            else:\n",
    "                greater_than_3[i] = [j]\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "#context dependent removal > 60 and rare tokens removal < 5\n",
    "count_list = list(chain.from_iterable([set(value) for value in greater_than_3.values()]))\n",
    "\n",
    "#document frequency of each token\n",
    "document_freq = FreqDist(count_list)\n",
    "final_document_freq = document_freq.copy()\n",
    "for key,val in document_freq.items():\n",
    "    if val > 60 or val < 5:\n",
    "        #storing valid keys only after removal of rare tokens and context dependent removal\n",
    "        final_document_freq.pop(key)\n",
    "        \n",
    "\n",
    "        \n",
    "rare_removal = {}\n",
    "for i,j in greater_than_3.items():\n",
    "        #creating date wise valid tokens\n",
    "        rare_removal[i] = [k for k in j if k in final_document_freq.keys()]\n",
    "        \n",
    "\n",
    "#stem the words\n",
    "stemmed_words = {}\n",
    "for i in rare_removal:\n",
    "    for j in rare_removal[i]:\n",
    "        if i in stemmed_words:\n",
    "            #stemming of the tokens date wise\n",
    "            stemmed_words[i].append(stem.stem(j))\n",
    "        else:\n",
    "             stemmed_words[i] = [stem.stem(j)]\n",
    "\n",
    "\n",
    "#count of tokens date wise\n",
    "uni_final = {}\n",
    "for i in stemmed_words:\n",
    "    #FreqDist to get count of each token\n",
    "    word_fd_uni = nltk.FreqDist(stemmed_words[i])\n",
    "    #storing date wise top 100 unigrams only alongwith its count\n",
    "    uni_final[i] = word_fd_uni.most_common(100)\n",
    "    \n",
    "\n",
    "    \n",
    "final_string_uni = ''\n",
    "#iterating inside uni_final\n",
    "for i in uni_final:\n",
    "    #generating final_string_uni in the required format\n",
    "    final_string_uni = final_string_uni + str(i) + ':' +  str(uni_final[i]) + '\\n'\n",
    "    \n",
    "    \n",
    "#writing the final_string_uni to the file sample_100uni.txt\n",
    "f3 = open(\"30759307_100uni.txt\" , 'w', encoding = 'utf8')\n",
    "f3.write(final_string_uni)\n",
    "f3.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we have created the file containing top 100 unigrams for each date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation is as follows - \n",
    "\n",
    "* In this step I have generated bigrams from the entire original tokens_dicti values hence the entire set of tokens(Not date wise). I have stored this in finder_new and then extracted the top 200 of it to store in the bigram_list. Hence bigram_list contains the top 200 bigrams extacted from all the tokens in the entire data. \n",
    "* Then I have created a list called vocab_final_list containing the valid bigrams with '_' between them. \n",
    "* Next I have used the stemmed_words dictionary created above to get just the valid unigrams. This dictionary contains valid unigrams and their dates hence I have appended just the unigrams into the list vocab_final_list. \n",
    "* Next I have created a set of this list to remove any repetitions using the set()\n",
    "* Next I have sorted this to get write_list as the final vocab using sorted().\n",
    "* Next I have created a string final_string_vocab to get the ouput in the required format. I have used range and len function to get the indexes/index of each token in vocab. \n",
    "* Finally I have written this string into the file sample_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOCAB.txt\n",
    "\n",
    "    \n",
    "# FINALLLLL DISHIIIII JAINNNN\n",
    "\n",
    "vocab_final_list = []\n",
    "vocab_final = {}\n",
    "all_words = list(chain.from_iterable(tokens_dicti.values()))\n",
    "finder_new = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "bigram_list = finder_new.nbest(bigram_measures.pmi,200)\n",
    "for x in bigram_list:\n",
    "    vocab_final_list.append(str(x[0]) + '_' + str(x[1]))\n",
    "    \n",
    "\n",
    "    \n",
    "for i,j in stemmed_words.items():\n",
    "    for x in j:\n",
    "        vocab_final_list.append(x)\n",
    "    \n",
    "vocab_final_set = set(vocab_final_list)\n",
    "write_list = sorted(vocab_final_set)\n",
    "\n",
    "\n",
    "final_string_vocab = ''\n",
    "for i in range(len(write_list)):\n",
    "        final_string_vocab = final_string_vocab + str(write_list[i]) + ':' + str(i) + '\\n'\n",
    "        \n",
    "\n",
    "        \n",
    "f4 = open(\"30759307_vocab.txt\" , 'w', encoding = 'utf8')\n",
    "f4.write(final_string_vocab)\n",
    "f4.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step I have generated a file called sample_vocab.txt containing the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generating CountVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I have to create a CountVector text file containing the date wise count of each token where the token is seen as its index value from the vocab file. Implementation is done as follows - \n",
    "\n",
    "* First I have created date wise bigrams from the original tokens_dicti dictionary. I have created all the possible bigrams in this and then I have sorted them. This gives us a dictionary d1 which contains date as key and a list of tuples containing bigrams and its count. \n",
    "* Next I have created a copy of d1 dictionary and created d2 dictionary to get only those bigrame date wise which are valid and present in the list bigram_list. I have used list comprehension to do so. \n",
    "* Next I have appended the valid bigrams date wise into the dictionary stemmed_words. Hence stemmed_words will finally contain date as key and a list of valid tokens(uni and bi grams) as its value. \n",
    "* Then I have created a file sample_countVec.txt to get the output in the desired format. For this I have first created a list of the final vocab's file. Then a new dictionary vocab_vector_dic is created that contains each vocab token as key and a counter as its value. \n",
    "* Next I have iterted inside stemmed_words dictionary to get the date wise index/counter of each token and its count. Hence d_idx is a list containing index of tokens as many times as the token was present. Using FreqDist will then give key as the token index and value as the frequency/count of that token. This is then stored in string_vector in the required format.\n",
    "* The string_vector is then written to the file sample_countVec.txt to get the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COUNT VECTOR\n",
    "\n",
    "d1 = {}\n",
    "\n",
    "#iterating inside original tokens_dicti\n",
    "for i in tokens_dicti:\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(tokens_dicti[i])\n",
    "    #creating bigrams from all tokens date wise\n",
    "    nope = bigram_measures.pmi\n",
    "    d1[i] = (sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0])))\n",
    "    \n",
    "    \n",
    "    \n",
    "d2 = d1.copy()\n",
    "\n",
    "#getting only the valid bigrams after comparing from the list bigram_list\n",
    "for i,j in d1.items():\n",
    "    d2[i] = [w for w in d1[i] if w[0] in bigram_list]\n",
    "\n",
    "\n",
    "for i,j in stemmed_words.items():\n",
    "    for y,z in d2.items():\n",
    "        if i == y:\n",
    "            for x in z:\n",
    "                #appending valid bigrams to stemmed_words\n",
    "                j.append(str(x[0][0]) + '_' + str(x[0][1]))\n",
    "\n",
    "\n",
    "#writing the data to the file sample_countVec.txt\n",
    "out_file = open(\"./30759307_countVec.txt\" , 'w')\n",
    "#creating the vocab_vector as a list of vocab\n",
    "vocab_vector = list(write_list)\n",
    "vocab_vector_dic = {}\n",
    "i = 0\n",
    "for w in vocab_vector:\n",
    "    #indexing each token in vocab starting from 0\n",
    "    vocab_vector_dic[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "string_vector = ''\n",
    "#iterating in stemmed_words dictionary \n",
    "for c,d in stemmed_words.items():\n",
    "    #adding the date into the string\n",
    "    string_vector = string_vector + str(c)\n",
    "    #creating d_idx to get occurences of each token index\n",
    "    d_idx = [vocab_vector_dic[w] for w in d]\n",
    "    #using freqdist to get count of each token index\n",
    "    for k,v in FreqDist(d_idx).items():\n",
    "        string_vector = string_vector + ',' + str(k) + ':' + str(v)\n",
    "    string_vector = string_vector + '\\n'\n",
    "\n",
    "#writing the string_vector to the file\n",
    "out_file.write(string_vector)\n",
    "out_file.close\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above step I have created a file sample_countVec.txt in the required format as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assessment I have created 4 files. For creation of sample_100bi.txt I have created bigrams date wise using collocations and pmi measure. The top 100 bigrams are taken into consideration for each date. For creating the sample_100uni.txt file I have first removed the stopwords from the original tokens_dicti dictionary date wise. Then I have removed the tokens with length less than 3. Then I have removed the rare tokens and context dependent tokens. Then I have stemmed the tokens and used FreqDist to count occurence of each token date wise. For creation of sample_vocab.txt file I have used the already generated valid unigrams and generated new bigrams generated from the entire list of tokens. I have then combined these two into a final vocab list where they are arranged alphabetically and contains index/counter. For creation of the file sample_countVec.txt I have found the valid bigrams and unigrams date wise and combined them together. Then used the vocab file to get index of each token and counted its occurence using FreqDist. This way the final file is also generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
